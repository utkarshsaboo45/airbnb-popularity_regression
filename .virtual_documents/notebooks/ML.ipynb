import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import altair as alt
import os
from sklearn import datasets
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import Ridge
from sklearn.impute import SimpleImputer

from sklearn.model_selection import (
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.ensemble import VotingRegressor
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, FunctionTransformer
from sklearn.tree import DecisionTreeRegressor, export_graphviz
from sklearn import datasets, ensemble
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostRegressor
from lightgbm.sklearn import LGBMRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn import set_config
from sklearn.ensemble import StackingRegressor
from sklearn.model_selection import GridSearchCV

import sys

if not sys.warnoptions:
    import warnings
    warnings.simplefilter("ignore")
alt.data_transformers.disable_max_rows()


get_ipython().run_line_magic("matplotlib", " inline")


data_df = pd.read_csv("AB_NYC_2019.csv")
data_df.head()


data_df.shape


data_df.info()





data_df.query("last_review.isna()")[["last_review", "reviews_per_month"]]


# remove the NaN from the target variable
data_df = data_df[~data_df['reviews_per_month'].isnull()]


data_df.info()


train_df, test_df = train_test_split(data_df, test_size = 0.3, random_state = 123)


X_train,y_train = train_df.drop(columns =["reviews_per_month"]),train_df["reviews_per_month"]
X_test, y_test = test_df.drop(columns=["reviews_per_month"]), test_df["reviews_per_month"]



train_df


categorical_features_eda = ["neighbourhood_group",
                            "neighbourhood",
                            "room_type"]



pd.DataFrame(train_df[categorical_features_eda].nunique())


train_df["year"] = pd.DatetimeIndex(train_df["last_review"]).year



reviews_over_years = alt.Chart(train_df, title = "Increase in reviews over time").mark_area().encode(
                        x = "year",
                        y = "count(reviews_per_month)",
                        color = "count(reviews_per_month)")
final_plot = reviews_over_years + reviews_over_years.mark_square()
final_plot


alt.Chart(train_df).mark_circle().encode(
    alt.X(alt.repeat("column"), type='quantitative'),
    alt.Y(alt.repeat("row"), type='quantitative'),
    color='Origin:N'
).properties(
    width=100,
    height=100
).repeat(
    row=["reviews_per_month"],
    column=["price", "minimum_nights", "calculated_host_listings_count","availability_365"]
).interactive()


lat_lon_hist = alt.Chart(train_df,
                         width=150,
                         height=200,
                         title="Dependence on latitude/longitude"
                        ).mark_bar().encode(
    x=alt.X(alt.repeat("repeat"),
            type="quantitative", 
            bin=alt.Bin(maxbins=30)
           ),
    y="count(reviews_per_month)"
).repeat(
    repeat=["latitude",
            "longitude"
           ],
    columns=2
    )


lat_lon_hist


cat_plot_room_type = alt.Chart(train_df).mark_boxplot().encode(
       x=alt.X("room_type"),
       y="reviews_per_month"
).properties(
  height=400, width=600,
  title='Distribution of No of reviews for each room type'
)

cat_plot_room_type


cat_plot_neighbourhood_group = alt.Chart(train_df).mark_boxplot(opacity=0.5).encode(
       x=alt.X("neighbourhood_group"),
       y="reviews_per_month"
).properties(
  height=400, width=600,
  title='Distribution of No of reviews for each neighbourhood group'
)

cat_plot_neighbourhood_group


# Extract month and year from the last_review
X_train['last_review_month'] = pd.DatetimeIndex(X_train['last_review']).month
X_train['last_review_year'] = pd.DatetimeIndex(X_train['last_review']).year
X_train = X_train.drop(columns = ["last_review"])



# Extract month and year from the last_review
X_test['last_review_month'] = pd.DatetimeIndex(X_test['last_review']).month
X_test['last_review_year'] = pd.DatetimeIndex(X_test['last_review']).year
X_test = X_test.drop(columns = ["last_review"])



# Converting the neighbourhoods occuring less than 86 times to "Others"
less_frequent_list = X_train["neighbourhood"].value_counts()[86:].index.tolist()
X_train.loc[X_train.query(
    "neighbourhood in @less_frequent_list"
).index, "neighbourhood"] = "Others"


# Converting the neighbourhoods occuring less than 86 times to "Others"
less_frequent_list = X_test["neighbourhood"].value_counts()[86:].index.tolist()
X_test.loc[X_test.query(
    "neighbourhood in @less_frequent_list"
).index, "neighbourhood"] = "Others"


# Converting the minimum_nights greater than 30 to 31 nights to reduce the number of columns for categorical features.
X_train.loc[X_train.query(
    "minimum_nights > 30"
).index, "minimum_nights"] = 31


# Converting the minimum_nights greater than 30 to 31 nights to reduce the number of columns for categorical features.
X_test.loc[X_test.query(
    "minimum_nights > 30"
).index, "minimum_nights"] = 31


for max in range(30, 366, 30):
    min = max - 30
    X_train.loc[X_train.query(
        "availability_365 > @min & availability_365 <= @max"
    ).index, "availability_365"] = max

X_train.loc[X_train.query(
    "availability_365 > @max & availability_365 <= 365"
    ).index, "availability_365"] = 365


for max in range(30, 366, 30):
    min = max - 30
    X_test.loc[X_test.query(
        "availability_365 > @min & availability_365 <= @max"
    ).index, "availability_365"] = max

X_test.loc[X_test.query(
    "availability_365 > @max & availability_365 <= 365"
).index, "availability_365"] = 365


numerical_features = [
    "price",
    "latitude",
    "longitude",
    "minimum_nights",
    "calculated_host_listings_count",
    "availability_365",
    "last_review_year"
]

categorical_features = [
    "neighbourhood_group",
    "neighbourhood",
    "room_type",
    "last_review_month"
]

text_features = [
    "name"
]

drop_features = [
    "id",
    "host_name",
    "host_id",
    "number_of_reviews"
]



assert len(X_test.columns)==len(
    text_features +
    drop_features +
    categorical_features +
    numerical_features)
print("All features included")


imp = SimpleImputer(strategy='constant')
func = FunctionTransformer(
    np.reshape, kw_args={"newshape": -1}
)
enc = CountVectorizer(max_features=30)

numeric_transformer = make_pipeline(
    StandardScaler()
)

categorical_transformer = make_pipeline(
    OneHotEncoder(handle_unknown="ignore", sparse=False)
)

text_transformer = make_pipeline(
    imp,
    func,
    enc
)


preprocessor = make_column_transformer(
    (numeric_transformer, numerical_features),
    (text_transformer, text_features),
    (categorical_transformer, categorical_features)
)


preprocessor.fit(X_train)


new_columns = (
    numerical_features +
    preprocessor.named_transformers_[
        "pipeline-2"
    ].named_steps[
        "countvectorizer"
    ].get_feature_names_out().tolist() +
    preprocessor.named_transformers_[
        "pipeline-3"
    ].get_feature_names_out().tolist()
)



X_train_transformed = preprocessor.fit_transform(X_train)


pd.DataFrame(data=X_train_transformed, columns=new_columns)


from sklearn.metrics import make_scorer


def mape(true, pred): # adapted from notes
    return 100.0 * np.mean(np.abs((pred - true) / true))


# make a scorer function that we can pass into cross-validation
mape_scorer = make_scorer(mape, greater_is_better=False)

scoring_metrics = {
    "neg RMSE": "neg_root_mean_squared_error",
    "r2": "r2",
    "mape": mape_scorer,
}


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores[i], std_scores[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


results = {}
dummy = DummyRegressor()
pipe_baseline = make_pipeline(preprocessor, dummy)
results["Dummy"] = mean_std_cross_val_scores(
    pipe_baseline,
    X_train,
    y_train,
    cv=10,
    return_train_score=True,
    scoring=scoring_metrics
)



pd.DataFrame(results)


param_grid = {"alpha": 10.0 ** np.arange(-2, 6, 1)}
results_lr = {}

best_param = 0.01
best_score = 0

for alpha_i in param_grid['alpha']:
    pipe_lr = make_pipeline(
        preprocessor,
        Ridge(alpha=alpha_i)
    )
    results_lr[alpha_i] = mean_std_cross_val_scores(
        pipe_lr,
        X_train,
        y_train,
        cv=5,
        return_train_score=True,
        scoring=scoring_metrics
)


pd.DataFrame(results_lr)


# Perform ridge with alpha = 1.00
pipe_lr_final = make_pipeline(
        preprocessor,
        Ridge(alpha=1.0))
results["Ridge"] = mean_std_cross_val_scores(
        pipe_lr_final,
        X_train,
        y_train,
        cv=5,
        return_train_score=True,
        scoring=scoring_metrics
)



pd.DataFrame(results)


reg_models = {
              "Random_Forest_reg": make_pipeline(preprocessor,
                                                 RandomForestRegressor(random_state=123)
                                            ),
              "XGBoost_reg": make_pipeline(preprocessor,
                                           XGBRegressor(verbosty=0)
                                           ),
              "lgbm_reg": make_pipeline(preprocessor,
                                        LGBMRegressor()
                                       )
}

for name, value in reg_models.items():
    results[name] = mean_std_cross_val_scores(value, X_train, y_train, cv=5,
                                              return_train_score=True, scoring=scoring_metrics
                                             )



pd.DataFrame(results)



param_grid_rf = {
    'randomforestregressor__max_depth': [80, 90, 100, 110],
    'randomforestregressor__max_features': [2, 3],
    'randomforestregressor__min_samples_leaf': [3, 4, 5],
    'randomforestregressor__n_estimators':[10, 100, 1000]
}

results_rf = {}

pipe_rf = make_pipeline(preprocessor, RandomForestRegressor())
grid_search = GridSearchCV(pipe_rf,
                           param_grid=param_grid_rf,
                           cv=5,
                           n_jobs=-1,
                           return_train_score=True,
                           scoring="r2")
results_rf = grid_search.fit(X_train, y_train)




pd.DataFrame(results_rf.cv_results_)[["mean_fit_time", 
                                      "mean_score_time",
                                      "param_xgbregressor__max_depth",
                                      "param_xgbregressor__n_estimators",
                                      "mean_test_score",
                                      "mean_train_score"]]


results_rf.best_params_


results_rf.best_score_


# Grid Search lgbm Regressor


param_grid_lgbm = {
    'lgbmregressor__n_estimators': [10, 100, 1000],
    'lgbmregressor__max_depth': [5, 10, 15]
}

results_lgbm = {}

pipe_lgbm = make_pipeline(preprocessor, LGBMRegressor())
grid_search = GridSearchCV(pipe_lgbm,
                           param_grid_lgbm,
                           cv=5,
                           n_jobs=-1,
                           return_train_score=True,
                           scoring="r2")
results_lgbm = grid_search.fit(X_train, y_train)




pd.DataFrame(results_lgbm.cv_results_)[["mean_fit_time", 
                                      "mean_score_time",
                                      "param_xgbregressor__max_depth",
                                      "param_xgbregressor__n_estimators",
                                      "mean_test_score",
                                      "mean_train_score"]]


results_lgbm.best_params_


results_lgbm.best_score_


param_grid_xgb = {
    "xgbregressor__n_estimators": [10, 100, 1000],
    "xgbregressor__max_depth": [5, 10, 15],
}

results_xgb = {}
pipe_xgb = make_pipeline(preprocessor, XGBRegressor(verbosity=0))
grid_search = GridSearchCV(pipe_xgb,
                           param_grid=param_grid_xgb,
                           cv=5,
                           n_jobs=-1,
                           return_train_score=True,
                           scoring="r2")
results_xgb = grid_search.fit(X_train, y_train)



pd.DataFrame(results_xgb.cv_results_)[["mean_fit_time", 
                                      "mean_score_time",
                                      "param_xgbregressor__max_depth",
                                      "param_xgbregressor__n_estimators",
                                      "mean_test_score",
                                      "mean_train_score"]]


results_xgb.best_params_


results_xgb.best_score_


scores = {
    "Random_Forest": results_rf.best_score_,
    "LGBM_reg": results_lgbm.best_score_,
    "XGBoost": results_xgb.best_score_
}

final_scores = pd.DataFrame.from_dict(scores, columns=['score'], orient="index")
final_scores


# Build a Stacking model with best estimators from each model.
pipe_rf_best = make_pipeline(
    preprocessor, RandomForestRegressor(
        max_depth: 90, 
        max_features: 3,
        min_samples_leaf: 3,
        n_estimators: 1000
    )
)
    
pipe_xgb_best = make_pipeline(
    preprocessor, XGBRegressor(
        max_depth=5, 
        n_estimators=100, 
        verbosity=0
    )
)
    
pipe_lgbm_best = make_pipeline(
    preprocessor, LGBMRegressor(
        max_depth= 15,
        n_estimators= 100
    )
)
    
regressors = {
    'random_forest': pipe_rf_best,
    'xbg_reg': pipe_xgb_best,
    'lgbm_reg': pipe_lgbm_best
}

stacking_reg = StackingClassifier(
    list(regressors.items()
        ), 
    final_estimator= pipe_lgbm_best
)

stacking_reg.fit(X_train, y_train)
    





pipe_lgbm_final = make_pipeline(preprocessor, LGBMRegressor(max_depth=15, n_estimators=100))
pipe_lgbm_final.fit(X_train, y_train)

import eli5

eli5.explain_weights(pipe_lgbm_final.named_steps["lgbmregressor"], feature_names=new_columns)



data={
    "Importance": pipe_lgbm_final.named_steps["lgbmregressor"].feature_importances_,
}
pd.DataFrame(data=data, index=new_columns,).sort_values(
    by="Importance", ascending=False
)[:10]





import shap

lgbm_explainer = shap.TreeExplainer(pipe_lgbm_final.named_steps["lgbmregressor"])

train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_transformed)

train_lgbm_shap_values


values = np.abs(train_lgbm_shap_values[1])
pd.DataFrame(data=values, index=new_columns, columns=["SHAP"]).sort_values(
    by="SHAP", ascending=False
)[:10]


X_train_transformed = pd.DataFrame(data=X_train_transformed, columns=new_columns)


shap.summary_plot(train_lgbm_shap_values, X_train_transformed, plot_type="bar")


pipe_lgbm_final.score(X_test, y_test)


X_test_transformed = pd.DataFrame(
    data=preprocessor.transform(X_test),
    columns=new_columns,
    index=X_test.index,
)


test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_transformed)


X_train_enc = X_train_transformed.round(3)
X_test_enc = X_test_transformed.round(3)


shap.force_plot(
    lgbm_explainer.expected_value,
    train_lgbm_shap_values[6],
    X_train_transformed.iloc[6, :],
    matplotlib=True,
)


shap.force_plot(
    lgbm_explainer.expected_value,
    test_lgbm_shap_values[6],
    X_test_transformed.iloc[6, :],
    matplotlib=True,
)


pd.DataFrame(results)


final_scores
